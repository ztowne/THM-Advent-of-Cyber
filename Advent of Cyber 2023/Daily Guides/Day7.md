# Day 7
## Log Analysis

In today's room, we will be doing some more log analysis. This time, we will be analyzing a proxy log and learn how to parse through them. 

Our learning objectives for the day are:

- Revisit log files and their importance
- Understand what a proxy is and break down the contents of a proxy log
- Build Linux command-line skills to parse logs manually
- Analyze a proxy log based on typical use cases

## What is a Proxy Server?

A proxy server, by definition, is an intermediary between your device and the internet. When you request information, your device connects to the proxy server instead of directly connecting to the target server. A proxy server offers enhanced visibility into network traffic and user activities, as it logs all web requests and responses. 

## Chopping Down the Proxy Log

Here, we will have a short refresher on some Linux commands to make parsing through this log easier. First, we can take a look at the log format for our dataset:

![Alt text](/Advent%20of%20Cyber%202023/Resources/logformat.png)

Based on this, we know we can split this log per column using ```cut```.

For example, a command we can use to see the 1st, 3rd, and 6th columns is:

```cut -d ' ' -f1,3,6 access.log```

Where

- "-d ' ' " uses a space as the delimiter
- -f is the position, and the numbers are the columns we are choosing
- access.log is our dataset we are working with in this lab

Next, we can use ```grep``` using a pipe ```|``` in order to search for specific strings in a file. We can also use ```head``` which will get the first specified number of entries. 
To get the first 5 connections made by a specific IP address, we use the following:

```grep 10.10.140.96 access.log | head -n 5```

![Alt text](/Advent%20of%20Cyber%202023/Resources/grepd7.png)

Now, we can use ```sort``` and ```uniq``` to get a list of unique domains accessed by all workstations.

- ```sort``` allows us to sory the lines of text files in ascending or descending order
- ```uniq``` allows us to filter and display unique lines from a sorted file

Along with using cut to retrieve the column as well as removes the port from ```domain:port```, our command for this is:

```cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort | uniq```

With these above commands, we can get a little granular with them using a couple options.

- Adding a ```-c``` to the ```uniq``` command with allows us to get a count of each domain accessed
- Adding a ```-n``` to ```sort``` allows us to sort based on the count of each domain

Our command now looks like this:

```cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort | uniq -c | sort -n```

## Hunting Down the Malicious Traffic

To start hunting fown the suspicious traffic, we are going to pull the list of the top domains accessed. Since our list is sorted in ascending order, the domains with the highest connection count are at the end of the list. Therefore, we will use ```tail -n 10``` instead of ```head```.

We will simply append this to our previously written command above:

```cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort | uniq -c | sort -n | tail -n 10```

![Alt text](/Advent%20of%20Cyber%202023/Resources/suspicioustraffic.png)

We see our top 10 domain connections, which all seemed to be owner by Microsoft - except: ```frostlings.bigbadstash.thm```.

We can use ```grep``` and ```head``` to see the first 10 connections made to it:

```grep frostlings.bigbadstash.thn access.log | head -n 10```

![Alt text](/Advent%20of%20Cyber%202023/Resources/domainconnectionsd7.png)

Now, we can see there is an odd string being passed to the ```goodies``` parameter among these requests. We can pull that data using grep and ```=``` as our delimeter:

```grep frostlings.bigbadstash.thn access.log | cut -d ' ' -f5 | cut -d '=' -f2 | head -n 10```

![Alt text](/Advent%20of%20Cyber%202023/Resources/d7outputb64.png)

We see that the data here seems to be encoded with ```Base64```. We can decode these strings by piping the output to a base64 command:

```grep frostlings.bigbadstash.thm access.log | cut -d ' ' -f5 | cut -d '=' -f2 | head -n 10 | base64 -d```

![Alt text](/Advent%20of%20Cyber%202023/Resources/dataexfild7.png)

Looking a this output, we can determine this might be a case of data exfiltration!

## Questions

At the conclusion of this lab, we will use the skills we learned today to answer the following questions:

1.  How many unique IP addresses are there?

Here is the command we used: 

 ```cut -d ' ' -f2 access.log | sort | uniq -c```

 Our answer is ```9```.


 2. How many unique domains were accessed by all workstations:

 Here is the command we used:

 ```cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort | uniq | wc -l```

Our answer here is ```111```.

3. What status code is generated by the HTTP Requests to the least accessed domain?

Our command for this question:

``` ```
Our answer here is ```503```

4. What is the source IP of the workstation that accessed the malicious domain?

Our command here is: 

```cut -d ' ' -f2,2 | grep "frostlings.bigbadstash.thm"```

Our answer for this question is ```10.10.185.225```

5. Lastly, what is the hidden flag?

Our command: 

```grep frostlings.bigbadstash.thm access.log | cut -d ' ' -f5 | cut -d '=' -f2 | head -n 10 | base64 -d | grep "THM"```

Our flag is ```THM{a_gift_for_you_awesome_analyst!}```

## Conclusion

Today we learned a variety of different Linux commands to help us navigate through logs and access the data we are the most interested in.